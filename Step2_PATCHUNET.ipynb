{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2304)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          295040      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           528         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            136         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            36          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            20          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           dense_7[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 8)            72          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16)           0           dense_8[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           272         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32)           0           dense_9[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           1056        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64)           0           dense_10[0][0]                   \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           4160        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           dense_11[0][0]                   \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          16512       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           dense_12[0][0]                   \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2304)         592128      concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 920,296\n",
      "Trainable params: 920,296\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\owner\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 12852 samples, validate on 3214 samples\n",
      "Epoch 1/50\n",
      "12852/12852 [==============================] - 3s 205us/step - loss: 0.0614 - val_loss: 0.0491\n",
      "Epoch 2/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0457 - val_loss: 0.0446\n",
      "Epoch 3/50\n",
      "12852/12852 [==============================] - 2s 124us/step - loss: 0.0429 - val_loss: 0.0429\n",
      "Epoch 4/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0417 - val_loss: 0.0420\n",
      "Epoch 5/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0410 - val_loss: 0.0415\n",
      "Epoch 6/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0406 - val_loss: 0.0411\n",
      "Epoch 7/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0403 - val_loss: 0.0409\n",
      "Epoch 8/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0401 - val_loss: 0.0407\n",
      "Epoch 9/50\n",
      "12852/12852 [==============================] - 2s 128us/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 10/50\n",
      "12852/12852 [==============================] - 2s 127us/step - loss: 0.0398 - val_loss: 0.0405\n",
      "Epoch 11/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0398 - val_loss: 0.0404\n",
      "Epoch 12/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0397 - val_loss: 0.0404\n",
      "Epoch 13/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0396 - val_loss: 0.0403\n",
      "Epoch 14/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0396 - val_loss: 0.0403\n",
      "Epoch 15/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0396 - val_loss: 0.0402\n",
      "Epoch 16/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0395 - val_loss: 0.0402\n",
      "Epoch 17/50\n",
      "12852/12852 [==============================] - 2s 129us/step - loss: 0.0394 - val_loss: 0.0403\n",
      "Epoch 18/50\n",
      "12852/12852 [==============================] - 2s 127us/step - loss: 0.0395 - val_loss: 0.0402\n",
      "Epoch 19/50\n",
      "12852/12852 [==============================] - 2s 132us/step - loss: 0.0394 - val_loss: 0.0401\n",
      "Epoch 20/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0393 - val_loss: 0.0401\n",
      "Epoch 21/50\n",
      "12852/12852 [==============================] - 2s 127us/step - loss: 0.0394 - val_loss: 0.0402\n",
      "Epoch 22/50\n",
      "12852/12852 [==============================] - 2s 132us/step - loss: 0.0393 - val_loss: 0.0402\n",
      "Epoch 23/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0393 - val_loss: 0.0402\n",
      "Epoch 24/50\n",
      "12852/12852 [==============================] - 2s 127us/step - loss: 0.0393 - val_loss: 0.0401\n",
      "Epoch 25/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0392 - val_loss: 0.0401\n",
      "Epoch 26/50\n",
      "12852/12852 [==============================] - 2s 128us/step - loss: 0.0392 - val_loss: 0.0401\n",
      "Epoch 27/50\n",
      "12852/12852 [==============================] - 2s 128us/step - loss: 0.0392 - val_loss: 0.0401\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0392 - val_loss: 0.0401\n",
      "Epoch 29/50\n",
      "12852/12852 [==============================] - 2s 124us/step - loss: 0.0392 - val_loss: 0.0401\n",
      "Epoch 30/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0392 - val_loss: 0.0400\n",
      "Epoch 31/50\n",
      "12852/12852 [==============================] - 2s 124us/step - loss: 0.0391 - val_loss: 0.0401\n",
      "Epoch 32/50\n",
      "12852/12852 [==============================] - 2s 123us/step - loss: 0.0391 - val_loss: 0.0400\n",
      "Epoch 33/50\n",
      "12852/12852 [==============================] - 2s 124us/step - loss: 0.0391 - val_loss: 0.0400\n",
      "Epoch 34/50\n",
      "12852/12852 [==============================] - 2s 130us/step - loss: 0.0390 - val_loss: 0.0401\n",
      "Epoch 35/50\n",
      "12852/12852 [==============================] - 2s 132us/step - loss: 0.0391 - val_loss: 0.0401\n",
      "Epoch 36/50\n",
      "12852/12852 [==============================] - 2s 127us/step - loss: 0.0391 - val_loss: 0.0401\n",
      "Epoch 37/50\n",
      "12852/12852 [==============================] - 2s 126us/step - loss: 0.0390 - val_loss: 0.0400\n",
      "Epoch 38/50\n",
      "12852/12852 [==============================] - 2s 131us/step - loss: 0.0390 - val_loss: 0.0400\n",
      "Epoch 39/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0390 - val_loss: 0.0400\n",
      "Epoch 40/50\n",
      "12852/12852 [==============================] - 2s 128us/step - loss: 0.0389 - val_loss: 0.0400\n",
      "Epoch 41/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0390 - val_loss: 0.0402\n",
      "Epoch 42/50\n",
      "12852/12852 [==============================] - 2s 125us/step - loss: 0.0390 - val_loss: 0.0400\n",
      "Epoch 00042: early stopping\n"
     ]
    }
   ],
   "source": [
    " #Omar M. Saad\n",
    "from keras.layers import Input, Dense, Dropout, concatenate, UpSampling1D, Flatten, MaxPooling1D, BatchNormalization, average, Conv1D\n",
    "from keras.models import Model\n",
    "import scipy.io\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import h5py\n",
    "from math import*\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "# reading input patches\n",
    "f = h5py.File('./Input_Patches.mat', 'r')\n",
    "dataNoise= f.get('dn_patch')\n",
    "\n",
    "# Random Permute for the patches for training\n",
    "ind = np.random.permutation(len(dataNoise))\n",
    "dataNoise = np.array(dataNoise)\n",
    "dataNoise = dataNoise[ind]\n",
    "\n",
    "\n",
    "\n",
    "INPUT_SIZE1 = dataNoise.shape[0]\n",
    "INPUT_SIZE2 = dataNoise.shape[1]\n",
    "#ENCODING_SIZE = 16\n",
    " \n",
    "input_img = Input(shape=(INPUT_SIZE2,))\n",
    "\n",
    "\n",
    "encoded1 = Dense(128,  activation='elu')(input_img)\n",
    "#encoded1 = BatchNormalization()(encoded1)\n",
    "#encoded1 = Dropout(0.01)(encoded1)\n",
    "\n",
    "encoded2 = Dense(64,  activation='elu')(encoded1)\n",
    "#encoded2 = BatchNormalization()(encoded2)\n",
    "#encoded2 = Dropout(0.01)(encoded2)\n",
    "\n",
    "encoded3 = Dense(32,  activation='elu' )(encoded2)\n",
    "#encoded3 = BatchNormalization()(encoded3)\n",
    "#encoded3 = Dropout(0.01)(encoded3)\n",
    "\n",
    "encoded4 = Dense(16,  activation='elu' )(encoded3)\n",
    "#encoded3 = BatchNormalization()(encoded3)\n",
    "#encoded4 = Dropout(0.01)(encoded4)\n",
    "\n",
    "encoded5 = Dense(8,  activation='elu' )(encoded4)\n",
    "#encoded5 = BatchNormalization()(encoded5)\n",
    "#encoded5 = Dropout(0.01)(encoded5)\n",
    "\n",
    "encoded6 = Dense(4,  activation='elu' )(encoded5)\n",
    "#encoded6 = BatchNormalization()(encoded6)\n",
    "#encoded6 = Dropout(0.01)(encoded6)\n",
    "\n",
    "# Decoder\n",
    "\n",
    "decoded1aa = Dense(4,  activation='elu' )(encoded6)\n",
    "#decoded1aa = BatchNormalization()(decoded1aa)\n",
    "decoded1aa = concatenate([decoded1aa,encoded6])\n",
    "#decoded1aa = Dropout(0.01)(decoded1aa)\n",
    "\n",
    "\n",
    "decoded1a = Dense(8,  activation='elu' )(decoded1aa)\n",
    "#decoded1a = BatchNormalization()(decoded1a)\n",
    "decoded1a = concatenate([decoded1a,encoded5])\n",
    "#decoded1a = Dropout(0.01)(decoded1a)\n",
    "\n",
    "decoded1 = Dense(16,  activation='elu' )(decoded1a)\n",
    "#decoded1 = BatchNormalization()(decoded1)\n",
    "decoded1 = concatenate([decoded1,encoded4])\n",
    "#decoded1 = Dropout(0.01)(decoded1)\n",
    "\n",
    "decoded2 = Dense(32,  activation='elu')(decoded1)\n",
    "#decoded2 = BatchNormalization()(decoded2)\n",
    "decoded2 = concatenate([decoded2,encoded3])\n",
    "#decoded2 = Dropout(0.01)(decoded2)\n",
    "\n",
    "decoded3 = Dense(64,  activation='elu' )(decoded2)\n",
    "#decoded3 = BatchNormalization()(decoded3)\n",
    "decoded3 = concatenate([decoded3,encoded2])\n",
    "#decoded3 = Dropout(0.01)(decoded3)\n",
    "\n",
    "decoded4 = Dense(128,  activation='elu' )(decoded3)\n",
    "#decoded4 = BatchNormalization()(decoded4)\n",
    "decoded4 = concatenate([decoded4,encoded1])\n",
    "#decoded4 = Dropout(0.01)(decoded4)\n",
    "\n",
    "#decoded = Flatten()(decoded3)\n",
    "decoded = Dense(INPUT_SIZE2, activation='linear')(decoded4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "autoencoder = Model(input_img, decoded)\n",
    "sgd = optimizers.adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=sgd, loss='mse')\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "# Addig stopping condition\n",
    "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5)\n",
    "mc=ModelCheckpoint('best_model.h5',monitor='val_loss',mode='min',save_best_only=True)\n",
    "\n",
    "\n",
    "batch = 128\n",
    "#tic()\n",
    "history = autoencoder.fit(dataNoise,dataNoise, epochs=50, batch_size=batch, shuffle=\"batch\",callbacks=[es,mc], validation_split = 0.2)\n",
    "#toc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "f = h5py.File('./Input_Patches.mat', 'r')\n",
    "dataNoise= f.get('dn_patch')\n",
    "\n",
    "out = model.predict(dataNoise)\n",
    "scipy.io.savemat('Output_Patches.mat', mdict={'out': out})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
